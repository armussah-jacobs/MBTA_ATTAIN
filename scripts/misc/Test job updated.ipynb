{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"tags": [],
				"jp-MarkdownHeadingCollapsed": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \n",
					"output_type": "stream"
				},
				{
					"output_type": "display_data",
					"data": {
						"text/markdown": "\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %timeout            Int           The number of minutes after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session.\n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0, 3.0 and 4.0. \n                                      Default: 2.0.\n    %reconnect          String        Specify a live session ID to switch/reconnect to the sessions.\n----\n\n## Selecting Session Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %session_type       String        Specify a session_type to be used. Supported values: streaming and etl.\n----\n\n## Glue Config Magic \n*(common across all session types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n    %%tags        Dictionary          Specify a json-formatted dictionary consisting of tags to use in the session.\n    \n    %%assume_role Dictionary, String  Specify a json-formatted dictionary or an IAM role ARN string to create a session \n                                      for cross account access.\n                                      E.g. {valid arn}\n                                      %%assume_role \n                                      'arn:aws:iam::XXXXXXXXXXXX:role/AWSGlueServiceRole' \n                                      E.g. {credentials}\n                                      %%assume_role\n                                      {\n                                            \"aws_access_key_id\" : \"XXXXXXXXXXXX\",\n                                            \"aws_secret_access_key\" : \"XXXXXXXXXXXX\",\n                                            \"aws_session_token\" : \"XXXXXXXXXXXX\"\n                                       }\n----\n\n                                      \n## Magic for Spark Sessions (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n    %matplot      Matplotlib figure   Visualize your data using the matplotlib library.\n                                      E.g. \n                                      import matplotlib.pyplot as plt\n                                      # Set X-axis and Y-axis values\n                                      x = [5, 2, 8, 4, 9]\n                                      y = [10, 4, 8, 5, 2]\n                                      # Create a bar chart \n                                      plt.bar(x, y) \n                                      # Show the plot\n                                      %matplot plt    \n    %plotly            Plotly figure  Visualize your data using the plotly library.\n                                      E.g.\n                                      import plotly.express as px\n                                      #Create a graphical figure\n                                      fig = px.line(x=[\"a\",\"b\",\"c\"], y=[1,3,2], title=\"sample figure\")\n                                      #Show the figure\n                                      %plotly fig\n\n  \n                \n----\n\n"
					},
					"metadata": {}
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n%extra_py_files s3://aws-glue-assets-992382490096-us-east-1/jar/delta-core_2.12-1.0.0.jar\n%extra_jars s3://aws-glue-assets-992382490096-us-east-1/jar/delta-core_2.12-1.0.0.jar\n%spark_conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql import SparkSession\nfrom delta.tables import DeltaTable\nfrom pyspark.sql.functions import lit, col\nfrom awsglue.dynamicframe import DynamicFrame\nimport boto3\nimport pandas as pd\nimport os\nfrom datetime import datetime\nimport shutil\nfrom tqdm import tqdm\nfrom datetime import datetime, timedelta\n\ns3 = boto3.client('s3')\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.8 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nExtra py files to be included:\ns3://aws-glue-assets-992382490096-us-east-1/jar/delta-core_2.12-1.0.0.jar\nExtra jars to be included:\ns3://aws-glue-assets-992382490096-us-east-1/jar/delta-core_2.12-1.0.0.jar\nPrevious Spark configuration: None\nSetting new Spark configuration to: spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\ns3://aws-glue-assets-992382490096-us-east-1/jar/delta-core_2.12-1.0.0.jar\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: 9402d851-2408-44b2-b218-4f130bde8739\nApplying the following default arguments:\n--glue_kernel_version 1.0.8\n--enable-glue-datacatalog true\n--extra-py-files s3://aws-glue-assets-992382490096-us-east-1/jar/delta-core_2.12-1.0.0.jar\n--extra-jars s3://aws-glue-assets-992382490096-us-east-1/jar/delta-core_2.12-1.0.0.jar\n--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\nWaiting for session 9402d851-2408-44b2-b218-4f130bde8739 to get into ready status...\nSession 9402d851-2408-44b2-b218-4f130bde8739 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Process to Refresh Bronze Table to Make All Silver Values = 0\n",
			"metadata": {
				"editable": true,
				"tags": [],
				"jp-MarkdownHeadingCollapsed": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "This step is done once to restart the process of creating values for the silver table.\nOnly run this when you want to restart the process from scartch",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# # Initialize the Spark session\n# spark = SparkSession.builder \\\n#     .appName(\"Delta Lake Upsert Data Aggregations\") \\\n#     .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n#     .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n#     .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\") \\\n#     .getOrCreate()\n\n# # Define S3 bucket paths\n# bronzetable = \"s3://aws-glue-assets-992382490096-us-east-1/glue-logs/cleaned_filepaths_delta/\"\n\n# delta_table = DeltaTable.forPath(spark, bronzetable)\n# spark_df = delta_table.toDF().filter(\"silver = 1\")\n# pandas_df = spark_df.toPandas()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# pandas_df.shape",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "(13280, 2)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# test_list = list(pandas_df.file_path)\n# len(test_list)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "13280\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# silvertable = \"s3://aws-glue-assets-992382490096-us-east-1/glue-logs/silver_table/\"\n# delta_table_exists = DeltaTable.isDeltaTable(spark, silvertable)\n\n# bronze_delta_table = DeltaTable.forPath(spark, bronzetable) \n# silver_delta_table = DeltaTable.forPath(spark, silvertable)\n\n# test_df = spark.createDataFrame([(path,) for path in test_list], [\"file_path\"])\n# test_df = test_df.withColumn(\"silver\", lit(0))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# bronze_delta_table.alias(\"target\").merge(\n#     test_df.alias(\"source\"),\n#     \"target.file_path = source.file_path\"\n#     ).whenMatchedUpdateAll().execute()\n\n# #Stop the Spark session\n# spark.stop()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Process Signal Data Files from MBTA and save them into a Table",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "target_date = datetime.now().strftime('%Y_%m_%d')\n\n# Initialize the Spark session\nspark = SparkSession.builder \\\n    .appName(\"Delta Lake Upsert Data Aggregations\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\") \\\n    .getOrCreate()\n\n# Define S3 bucket paths\nbronzetable = \"s3://aws-glue-assets-992382490096-us-east-1/glue-logs/cleaned_filepaths_delta/\"\n\ndelta_table = DeltaTable.forPath(spark, bronzetable)\nspark_df = delta_table.toDF().filter(\"silver = 0\").filter(\n    (col(\"file_path\").contains(target_date)) & (~col(\"file_path\").contains(\"_detail\"))\n)\n# spark_df_filtered = spark_df.filter(col(\"file_path\").contains(\"2025_03_27\"))\npandas_df = spark_df.toPandas()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "pandas_df.shape",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "(205, 2)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# test_list = list(pandas_df.file_path)\n# len(test_list)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Define the patterns you're looking for\npatterns = [\n\"ARL0127\",\"ARL0156\",\"ARL0219\",\"ARL0562\",\"ARL2620\",\"CAM0636\",\n\"CAM4873\",\"MAL0001\",\"MAL0002\",\"MAL0003\",\"MAL0004\",\"MAL0005\",\n\"MAL0006\",\"MAL0007\",\"MALD0001\",\"MALD0002\",\"MALD0003\",\"MALD0004\",\n\"MALD0005\",\"MALD0006\",\"MALD0007\",\"SOM0315\",\"SOM0603\",\"SOM0827\",\n\"ARL004\",\"ARL001\",\"ARL002\",\"ARL003\",\"ARL005\",\"CAM015\",\n\"CAM018\",\"MAL001\",\"MAL002\",\"MAL003\",\"MAL004\",\"MAL006\",\n\"MAL007\",\"MAL009\",\"MAL001\",\"MAL002\",\"MAL003\",\"MAL004\",\n\"MAL006\",\"MAL007\",\"MAL009\",\"SOM001\",\"SOM002\",\"SOM006\"\n]\n\n# Function to handle pagination and filter files\ndef list_and_filter_files(bucket_name, prefix, target_date):\n    paginator = s3.get_paginator('list_objects_v2')\n    filtered_file_paths = []\n    \n    # Iterate over paginated results\n    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n        if 'Contents' in page:\n            for obj in page['Contents']:\n                file_path = obj['Key']\n                \n                # Filter for files containing the target date, matching patterns, and NOT containing \"_detail\"\n                if (target_date in file_path) and \\\n                   any(pattern in file_path for pattern in patterns) and \\\n                   (\"_detail\" not in file_path):\n                    filtered_file_paths.append(f\"s3://{bucket_name}/{file_path}\")\n    \n    return filtered_file_paths\n\n# Specify the S3 bucket and prefix (if any)\nbucket_name = 'mbta-tsp-signal'\nprefix = 'csv/'\n\n# Get the filtered file paths for the target date\nfile_paths = list_and_filter_files(bucket_name, prefix, target_date)\nprint(f\"Filtered File Paths: {len(file_paths)} found\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "Filtered File Paths: 106 found\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def find_common_items(my_list, spark_df, column_name):\n    list_df = spark.createDataFrame([(item,) for item in my_list], [\"item\"])\n    common_df = list_df.join(spark_df, list_df.item == col(column_name), \"inner\")\n    common_items = [row.item for row in common_df.select(\"item\").distinct().collect()]\n    return common_items\n\ncommon_items = find_common_items(file_paths, spark_df, \"file_path\")\n# print(common_items)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "def download_files(file_paths, local_dir):\n    if not os.path.exists(local_dir):\n        os.makedirs(local_dir)\n    for file_path in tqdm(file_paths, desc=\"Downloading files\"):  # Add tqdm here\n        bucket_name = file_path.split('/')[2]\n        key = '/'.join(file_path.split('/')[3:])\n        local_filename = os.path.join(local_dir, os.path.basename(file_path))\n        \n        response = s3.get_object(Bucket=bucket_name, Key=key)\n        with open(local_filename, 'wb') as file:\n            file.write(response['Body'].read())",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def process_files(file_paths):\n    local_directory = '/tmp/downloaded_files/'\n    download_files(file_paths, local_directory)\n\n    df_list = []\n    for root, dirs, files in os.walk(local_directory):\n        for file in tqdm(files, desc=\"Processing files\"):\n            file_path = os.path.join(local_directory, os.path.basename(file))\n            try:\n                df = pd.read_csv(file_path, header=None, names=['time', 'event', 'param'])\n                df['intersection_id'] = file.split('_')[0]\n                df['date'] = datetime.strptime(f\"{file.split('_')[3]}_{file.split('_')[4]}_{file.split('_')[5]}\", '%Y_%m_%d')\n                df['time'] = pd.to_datetime(df['time']) \n            except Exception as e:\n                print(f\"Skipping file {file_path}: Could not read as CSV. Error: {e}\")\n                continue\n            except pd.errors.EmptyDataError:\n                print(f\"Skipping empty file: {file_path}\")\n                continue\n            df_list.append(df)\n    for filename in os.listdir(local_directory):\n        file_path = os.path.join(local_directory, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n    return pd.concat(df_list, ignore_index=True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# --- Process in Chunks ---\nchunk_size = 10\nsilvertable = \"s3://aws-glue-assets-992382490096-us-east-1/glue-logs/silver_table/\"\ndelta_table_exists = DeltaTable.isDeltaTable(spark, silvertable)\n\nbronze_delta_table = DeltaTable.forPath(spark, bronzetable) \nsilver_delta_table = DeltaTable.forPath(spark, silvertable)\n\nfor i in range(0, len(common_items), chunk_size):\n    chunk = common_items[i:i + chunk_size]\n    print(f\"Processing chunk {i // chunk_size + 1} of {len(common_items) // chunk_size + 1}\")\n    \n    # Process the chunk\n    combined_df = process_files(chunk)\n    combined_df = spark.createDataFrame(combined_df)\n\n    # Perform UPSERT (MERGE)\n    silver_delta_table.alias(\"target\").merge(\n        combined_df.alias(\"source\"),\n        \"target.intersection_id = source.intersection_id AND target.time = source.time AND target.event = source.event\"\n        ).whenNotMatchedInsertAll().execute()\n    \n    test_df = spark.createDataFrame([(path,) for path in chunk], [\"file_path\"])\n    test_df = test_df.withColumn(\"silver\", lit(1))\n        \n    bronze_delta_table.alias(\"target\").merge(\n        test_df.alias(\"source\"),\n        \"target.file_path = source.file_path\"\n        ).whenMatchedUpdateAll().execute()\n\n#Stop the Spark session\nspark.stop()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# # --- Process in Chunks ---\n# chunk_size = 100\n# silvertable = \"s3://aws-glue-assets-992382490096-us-east-1/glue-logs/silver_table/\"\n# delta_table_exists = DeltaTable.isDeltaTable(spark, silvertable)\n\n# bronze_delta_table = DeltaTable.forPath(spark, bronzetable) \n# silver_delta_table = DeltaTable.forPath(spark, silvertable)\n\n# for i in range(0, len(common_items), chunk_size):\n#     chunk = common_items[i:i + chunk_size]\n#     print(f\"Processing chunk {i // chunk_size + 1} of {len(common_items) // chunk_size + 1}\")\n    \n#     # Process the chunk\n#     combined_df = process_files(chunk)\n#     combined_df = spark.createDataFrame(combined_df)\n\n#     # Perform UPSERT (MERGE)\n#     silver_delta_table.alias(\"target\").merge(\n#         combined_df.alias(\"source\"),\n#         \"target.intersection_id = source.intersection_id AND target.time = source.time AND target.event = source.event\"\n#         ).whenNotMatchedInsertAll().execute()\n#     # combined_df.write.format(\"delta\").mode(\"overwrite\").save(silvertable)\n    \n#     test_df = spark.createDataFrame([(path,) for path in chunk], [\"file_path\"])\n#     test_df = test_df.withColumn(\"silver\", lit(1))\n        \n#     bronze_delta_table.alias(\"target\").merge(\n#         test_df.alias(\"source\"),\n#         \"target.file_path = source.file_path\"\n#         ).whenMatchedUpdateAll().execute()\n\n# #Stop the Spark session\n# spark.stop()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "Processing chunk 1 of 2\nDownloading files: 100%|##########| 100/100 [00:06<00:00, 14.99it/s]\nProcessing files: 100%|##########| 100/100 [00:06<00:00, 15.28it/s]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}