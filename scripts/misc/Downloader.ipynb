{"metadata":{"kernelspec":{"name":"glue_pyspark","display_name":"Glue PySpark","language":"python"},"language_info":{"name":"Python_Glue_Session","mimetype":"text/x-python","codemirror_mode":{"name":"python","version":3},"pygments_lexer":"python3","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n","metadata":{"editable":true,"trusted":true}},{"cell_type":"markdown","source":"#### Optional: Run this cell to see available notebook commands (\"magics\").\n","metadata":{"editable":true,"trusted":true}},{"cell_type":"code","source":"%help","metadata":{"trusted":true,"editable":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"####  Run this cell to set up and start your interactive session.\n","metadata":{"editable":true,"trusted":true}},{"cell_type":"code","source":"%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)","metadata":{"trusted":true,"editable":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: 5149e686-4b61-4322-a0a7-d38a3119af37\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\nWaiting for session 5149e686-4b61-4322-a0a7-d38a3119af37 to get into ready status...\nSession 5149e686-4b61-4322-a0a7-d38a3119af37 has been created.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import necessary libraries\nimport boto3\nimport zipfile\nimport os\nfrom io import BytesIO","metadata":{"trusted":true,"tags":[]},"execution_count":2,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import boto3\nfrom datetime import datetime, timedelta\n\n# Initialize Boto3 client for S3\ns3_client = boto3.client('s3')\n\n# Define the patterns you're looking for\npatterns = [\n\"CAM0557\",\n\"CAM4417\",\n\"CAM0636\",\n\"CAM4873\",\n\"ARL0156\",\n\"ARL0219\",\n\"ARL0562\",\n\"ARL0127\",\n\"ARL2620\",\n\"CAM0794\",\n\"SOM0315\",\n\"SOM0603\",\n\"SOM0827\"\n]\n\n# Function to handle pagination and filter files\ndef list_and_filter_files(bucket_name, prefix, start_date, end_date):\n    paginator = s3_client.get_paginator('list_objects_v2')\n    filtered_file_paths = []\n\n    # Convert start_date and end_date to datetime objects\n    start_date = datetime.strptime(start_date, '%Y_%m_%d')\n    end_date = datetime.strptime(end_date, '%Y_%m_%d')\n\n    # Generate a list of target dates within the range\n    target_dates = [(start_date + timedelta(days=x)).strftime('%Y_%m_%d') \n                    for x in range((end_date - start_date).days + 1)]\n\n    # Iterate over paginated results\n    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n        if 'Contents' in page:\n            for obj in page['Contents']:\n                file_path = obj['Key']\n\n                # Filter for files containing any of the target dates and matching patterns\n                if any(target_date in file_path for target_date in target_dates) and \\\n                   any(pattern in file_path for pattern in patterns):\n                    filtered_file_paths.append(f\"s3://{bucket_name}/{file_path}\")\n\n    return filtered_file_paths\n\n# Example usage:\nbucket_name = 'mbta-tsp-signal'\nprefix = 'csv/'\nstart_date = '2025_01_01'\nend_date = '2025_02_10'\n\nfile_paths = list_and_filter_files(bucket_name, prefix, start_date, end_date)\nprint(f\"Filtered File Paths: {len(file_paths)} found\")","metadata":{"trusted":true,"tags":[]},"execution_count":3,"outputs":[{"name":"stdout","text":"Filtered File Paths: 17616 found\n","output_type":"stream"}]},{"cell_type":"code","source":"# import boto3\n\n# # Initialize Boto3 client for S3\n# s3_client = boto3.client('s3')\n\n# # Define the patterns you're looking for\n# patterns = [\n#     'BRKL0001', 'BRKL0002', 'BRKL0003', 'BRKL0004', 'BRKL0005', 'BRKL0006',\n#     'MALD0001', 'MALD0002', 'MALD0003', 'MALD0004', 'MALD0005', 'MALD0006',\n#     'MALD0007', 'SMVL0001'\n# ]\n\n# # Function to handle pagination and filter files\n# def list_and_filter_files(bucket_name, prefix, target_date='2024_10_03'):\n#     paginator = s3_client.get_paginator('list_objects_v2')\n#     filtered_file_paths = []\n    \n#     # Iterate over paginated results\n#     for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n#         if 'Contents' in page:\n#             for obj in page['Contents']:\n#                 file_path = obj['Key']\n                \n#                 # Filter for files containing the target date and matching patterns for 08:00 or 16:00\n#                 if (target_date in file_path) and \\\n#                    ('_0800.csv' in file_path or '_1600.csv' in file_path) and \\\n#                    any(pattern in file_path for pattern in patterns):\n#                     filtered_file_paths.append(f\"s3://{bucket_name}/{file_path}\")\n    \n#     return filtered_file_paths\n\n# # Specify the S3 bucket and prefix (if any)\n# bucket_name = 'mbta-tsp-signal'\n# prefix = 'csv/'\n\n# # Get the filtered file paths for the target date\n# file_paths = list_and_filter_files(bucket_name, prefix, target_date='2024_10_03')\n# print(f\"Filtered File Paths: {len(file_paths)} found\")\n","metadata":{"trusted":true,"tags":[]},"execution_count":7,"outputs":[{"name":"stdout","text":"Filtered File Paths: 28 found\n","output_type":"stream"}]},{"cell_type":"code","source":"filtered_s3_paths = [path for path in file_paths if not path.endswith('detail.csv')]\nlen(filtered_s3_paths)","metadata":{"trusted":true,"tags":[]},"execution_count":5,"outputs":[{"name":"stdout","text":"8808\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to download files from S3 without extra libraries\ndef download_files(file_paths, local_dir):\n    if not os.path.exists(local_dir):\n        os.makedirs(local_dir)\n    for file_path in file_paths:\n        bucket_name = file_path.split('/')[2]\n        key = '/'.join(file_path.split('/')[3:])\n        local_filename = os.path.join(local_dir, os.path.basename(file_path))\n        \n        # Use GetObject to download the file\n        response = s3_client.get_object(Bucket=bucket_name, Key=key)\n        \n        # Save the file locally\n        with open(local_filename, 'wb') as file:\n            file.write(response['Body'].read())\n        print(f\"Downloaded: {local_filename}\")\n\n# Specify local directory to store files\nlocal_directory = '/tmp/downloaded_files/'\n\n# Download the files\ndownload_files(file_paths, local_directory)\n","metadata":{"trusted":true,"tags":[]},"execution_count":9,"outputs":[{"name":"stdout","text":"Downloaded: /tmp/downloaded_files/BRKL0001_SIEM_172.20.10.13_2024_10_03_0800.csv\nDownloaded: /tmp/downloaded_files/BRKL0001_SIEM_172.20.10.13_2024_10_03_1600.csv\nDownloaded: /tmp/downloaded_files/BRKL0002_SIEM_172.20.10.23_2024_10_03_0800.csv\nDownloaded: /tmp/downloaded_files/BRKL0002_SIEM_172.20.10.23_2024_10_03_1600.csv\nDownloaded: /tmp/downloaded_files/BRKL0003_SIEM_172.20.10.33_2024_10_03_0800.csv\nDownloaded: /tmp/downloaded_files/BRKL0003_SIEM_172.20.10.33_2024_10_03_1600.csv\nDownloaded: /tmp/downloaded_files/BRKL0004_SIEM_172.20.10.43_2024_10_03_0800.csv\nDownloaded: /tmp/downloaded_files/BRKL0004_SIEM_172.20.10.43_2024_10_03_1600.csv\nDownloaded: /tmp/downloaded_files/BRKL0005_SIEM_172.20.10.53_2024_10_03_0800.csv\nDownloaded: /tmp/downloaded_files/BRKL0005_SIEM_172.20.10.53_2024_10_03_1600.csv\nDownloaded: /tmp/downloaded_files/BRKL0006_SIEM_172.20.10.63_2024_10_03_0800.csv\nDownloaded: /tmp/downloaded_files/BRKL0006_SIEM_172.20.10.63_2024_10_03_1600.csv\nDownloaded: /tmp/downloaded_files/MALD0001_ECON_10.99.16.8_2024_10_03_0800.csv\nDownloaded: /tmp/downloaded_files/MALD0001_ECON_10.99.16.8_2024_10_03_1600.csv\nDownloaded: /tmp/downloaded_files/MALD0002_ECON_10.99.16.40_2024_10_03_0800.csv\nDownloaded: /tmp/downloaded_files/MALD0002_ECON_10.99.16.40_2024_10_03_1600.csv\nDownloaded: /tmp/downloaded_files/MALD0003_ECON_10.99.16.72_2024_10_03_0800.csv\nDownloaded: /tmp/downloaded_files/MALD0003_ECON_10.99.16.72_2024_10_03_1600.csv\nDownloaded: /tmp/downloaded_files/MALD0004_ECON_10.99.16.104_2024_10_03_0800.csv\nDownloaded: /tmp/downloaded_files/MALD0004_ECON_10.99.16.104_2024_10_03_1600.csv\nDownloaded: /tmp/downloaded_files/MALD0005_ECON_10.99.16.136_2024_10_03_0800.csv\nDownloaded: /tmp/downloaded_files/MALD0005_ECON_10.99.16.136_2024_10_03_1600.csv\nDownloaded: /tmp/downloaded_files/MALD0006_ECON_10.99.16.168_2024_10_03_0800.csv\nDownloaded: /tmp/downloaded_files/MALD0006_ECON_10.99.16.168_2024_10_03_1600.csv\nDownloaded: /tmp/downloaded_files/MALD0007_ECON_10.99.16.200_2024_10_03_0800.csv\nDownloaded: /tmp/downloaded_files/MALD0007_ECON_10.99.16.200_2024_10_03_1600.csv\nDownloaded: /tmp/downloaded_files/SMVL0001_SIEM_10.99.17.40_2024_10_03_0800.csv\nDownloaded: /tmp/downloaded_files/SMVL0001_SIEM_10.99.17.40_2024_10_03_1600.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to zip the downloaded files\ndef zip_files(local_dir, zip_filename):\n    zip_buffer = BytesIO()\n    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n        for root, dirs, files in os.walk(local_dir):\n            for file in files:\n                file_path = os.path.join(root, file)\n                zip_file.write(file_path, os.path.basename(file_path))\n    return zip_buffer\n\n# Zip the files\nzip_buffer = zip_files(local_directory, 'downloaded_files.zip')\n\n# Upload the zip file to S3\noutput_bucket = 'aws-glue-assets-992382490096-us-east-1'\noutput_key = 'glue-logs/downloaded_files.zip'\ns3_client.put_object(Bucket=output_bucket, Key=output_key, Body=zip_buffer.getvalue())\nprint(f\"Uploaded zip to s3://{output_bucket}/{output_key}\")\n","metadata":{"trusted":true,"tags":[]},"execution_count":10,"outputs":[{"name":"stdout","text":"Uploaded zip to s3://aws-glue-assets-992382490096-us-east-1/glue-logs/downloaded_files.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}