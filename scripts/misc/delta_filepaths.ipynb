{"metadata":{"kernelspec":{"name":"glue_pyspark","display_name":"Glue PySpark","language":"python"},"language_info":{"name":"Python_Glue_Session","mimetype":"text/x-python","codemirror_mode":{"name":"python","version":3},"pygments_lexer":"python3","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n","metadata":{"editable":true,"trusted":true}},{"cell_type":"markdown","source":"#### Optional: Run this cell to see available notebook commands (\"magics\").\n","metadata":{"editable":true,"trusted":true}},{"cell_type":"code","source":"%help","metadata":{"trusted":true,"editable":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/markdown":"\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %timeout            Int           The number of minutes after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session.\n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0, 3.0 and 4.0. \n                                      Default: 2.0.\n    %reconnect          String        Specify a live session ID to switch/reconnect to the sessions.\n----\n\n## Selecting Session Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %session_type       String        Specify a session_type to be used. Supported values: streaming and etl.\n----\n\n## Glue Config Magic \n*(common across all session types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n    %%tags        Dictionary          Specify a json-formatted dictionary consisting of tags to use in the session.\n    \n    %%assume_role Dictionary, String  Specify a json-formatted dictionary or an IAM role ARN string to create a session \n                                      for cross account access.\n                                      E.g. {valid arn}\n                                      %%assume_role \n                                      'arn:aws:iam::XXXXXXXXXXXX:role/AWSGlueServiceRole' \n                                      E.g. {credentials}\n                                      %%assume_role\n                                      {\n                                            \"aws_access_key_id\" : \"XXXXXXXXXXXX\",\n                                            \"aws_secret_access_key\" : \"XXXXXXXXXXXX\",\n                                            \"aws_session_token\" : \"XXXXXXXXXXXX\"\n                                       }\n----\n\n                                      \n## Magic for Spark Sessions (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n    %matplot      Matplotlib figure   Visualize your data using the matplotlib library.\n                                      E.g. \n                                      import matplotlib.pyplot as plt\n                                      # Set X-axis and Y-axis values\n                                      x = [5, 2, 8, 4, 9]\n                                      y = [10, 4, 8, 5, 2]\n                                      # Create a bar chart \n                                      plt.bar(x, y) \n                                      # Show the plot\n                                      %matplot plt    \n    %plotly            Plotly figure  Visualize your data using the plotly library.\n                                      E.g.\n                                      import plotly.express as px\n                                      #Create a graphical figure\n                                      fig = px.line(x=[\"a\",\"b\",\"c\"], y=[1,3,2], title=\"sample figure\")\n                                      #Show the figure\n                                      %plotly fig\n\n  \n                \n----\n\n"},"metadata":{}}]},{"cell_type":"markdown","source":"####  Run this cell to set up and start your interactive session.\n","metadata":{"editable":true,"trusted":true}},{"cell_type":"code","source":"%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n%spark_conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\n%additional_python_modules s3://aws-glue-assets-992382490096-us-east-1/jar/delta-core_2.12-1.0.0.jar\n%extra_jars s3://aws-glue-assets-992382490096-us-east-1/jar/delta-core_2.12-1.0.0.jar\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)","metadata":{"trusted":true,"editable":true},"execution_count":7,"outputs":[{"name":"stderr","text":"You are already connected to a glueetl session 143fe9bd-6d27-4291-8a35-37c163bc0982.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n","output_type":"stream"},{"name":"stdout","text":"Current idle_timeout is 2880 minutes.\nidle_timeout has been set to 2880 minutes.\n","output_type":"stream"},{"name":"stderr","text":"You are already connected to a glueetl session 143fe9bd-6d27-4291-8a35-37c163bc0982.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n","output_type":"stream"},{"name":"stdout","text":"Setting Glue version to: 3.0\n","output_type":"stream"},{"name":"stderr","text":"You are already connected to a glueetl session 143fe9bd-6d27-4291-8a35-37c163bc0982.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n","output_type":"stream"},{"name":"stdout","text":"Previous worker type: G.1X\nSetting new worker type to: G.1X\n","output_type":"stream"},{"name":"stderr","text":"You are already connected to a glueetl session 143fe9bd-6d27-4291-8a35-37c163bc0982.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n","output_type":"stream"},{"name":"stdout","text":"Previous number of workers: 5\nSetting new number of workers to: 5\n","output_type":"stream"},{"name":"stderr","text":"You are already connected to a glueetl session 143fe9bd-6d27-4291-8a35-37c163bc0982.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n","output_type":"stream"},{"name":"stdout","text":"Previous session type: glueetl\nSetting new session type to ETL\nPrevious Spark configuration: spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\nSetting new Spark configuration to: spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\n","output_type":"stream"},{"name":"stderr","text":"You are already connected to a glueetl session 143fe9bd-6d27-4291-8a35-37c163bc0982.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n","output_type":"stream"},{"name":"stdout","text":"Additional python modules to be included:\ns3://aws-glue-assets-992382490096-us-east-1/jar/delta-core_2.12-1.0.0.jar\n","output_type":"stream"},{"name":"stderr","text":"You are already connected to a glueetl session 143fe9bd-6d27-4291-8a35-37c163bc0982.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n","output_type":"stream"},{"name":"stdout","text":"Extra jars to be included:\ns3://aws-glue-assets-992382490096-us-east-1/jar/delta-core_2.12-1.0.0.jar\ns3://aws-glue-assets-992382490096-us-east-1/jar/delta-core_2.12-1.0.0.jar\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql import SparkSession\nfrom delta.tables import DeltaTable\nimport boto3\n\ns3 = boto3.client('s3')\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\n\n# Initialize the Spark session\nspark = SparkSession.builder \\\n    .appName(\"Delta Lake Upsert Data Aggregations\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\") \\\n    .getOrCreate()\n\n# Define S3 bucket paths\noutput_path = \"s3://aws-glue-assets-992382490096-us-east-1/glue-logs/cleaned_filepaths_delta/\"\n\n# Bucket and path configurations\nbucket_name = 'mbta-tsp-signal'\ncsv_path_prefix = 'csv'\noutput_bucket = 'aws-glue-assets-992382490096-us-east-1'\noutput_key = 'glue-logs/file_paths.parquet'\n\n# Function to list all CSV files recursively\ndef list_csv_files(bucket, prefix):\n    result = []\n    paginator = s3.get_paginator('list_objects_v2')\n    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n        for obj in page.get('Contents', []):\n            if obj['Key'].endswith('.csv'):\n                result.append(f's3://{bucket}/{obj[\"Key\"]}')\n    return result\n\n# List CSV files\ncsv_file_paths = list_csv_files(bucket_name, csv_path_prefix)\nprint(f\"Number of CSV files found: {len(csv_file_paths)}\")","metadata":{"trusted":true,"tags":[]},"execution_count":8,"outputs":[{"name":"stdout","text":"ModuleNotFoundError: No module named 'delta'\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"end_time = time.time()\nelapsed_time = end_time - start_time\n\nprint(f\"Total time taken for the operation: {elapsed_time:.2f} seconds\")\n\n# Commit the job after completion\njob.commit()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}